#!/usr/bin/env python3
"""
uQCme - Microbial Quality Control Web Application

A Streamlit web application for visualizing and exploring microbial QC results
generated by the uQCme CLI tool. This module focuses on data tables and
interactive data exploration, with plotting functionality handled by plot.py.
"""

import os
import sys
import streamlit as st
from streamlit.web import cli as stcli
import pandas as pd
import yaml
from pathlib import Path
from typing import Dict, Any, Optional
from uQCme.plot import QCPlotter, get_available_metrics
from uQCme.core.loader import load_data_from_config, load_config_from_file
from uQCme.core.engine import QCProcessor
from uQCme.core.config import UQCMeConfig, DataInput
from uQCme.core.exceptions import ConfigError, DataLoadError


class QCDashboard:
    """Main class for the QC dashboard application."""

    def __init__(self, config_path: Optional[str]):
        """Initialize the dashboard with configuration."""
        self.config_path = config_path
        self.config: UQCMeConfig = self._load_config(config_path)
        self.data: pd.DataFrame = pd.DataFrame()
        self.mapping: Dict[str, Any] = {}
        self.qc_rules: pd.DataFrame = pd.DataFrame()
        self.qc_tests: pd.DataFrame = pd.DataFrame()
        self.plotter: QCPlotter = QCPlotter(self.config)
        
    def _load_config(self, config_path: Optional[str]) -> UQCMeConfig:
        """Load configuration from YAML file or use defaults."""
        if config_path:
            try:
                return load_config_from_file(config_path)
            except ConfigError as e:
                st.error(f"Configuration Error: {e}")
                st.stop()
            except Exception as e:
                st.error(f"Unexpected error loading config: {e}")
                st.stop()
        else:
            return self._get_default_config()

    def _get_default_config(self) -> UQCMeConfig:
        """Get default configuration using bundled files."""
        defaults_dir = Path(__file__).parent / 'defaults'
        config_path = defaults_dir / 'config.yaml'
        
        try:
            config = load_config_from_file(str(config_path))
        except ConfigError as e:
            st.error(f"Error loading default config: {e}")
            st.stop()
            
        # Update paths to point to the defaults directory if local files don't exist
        if config.app and config.app.input:
            inp = config.app.input
            # Update mapping, rules, tests to absolute paths in defaults dir
            if not os.path.exists(inp.mapping):
                inp.mapping = str(defaults_dir / Path(inp.mapping).name)
            if not os.path.exists(inp.qc_rules):
                inp.qc_rules = str(defaults_dir / Path(inp.qc_rules).name)
            if not os.path.exists(inp.qc_tests):
                inp.qc_tests = str(defaults_dir / Path(inp.qc_tests).name)
                    
        if config.qc and config.qc.input:
            inp = config.qc.input
            # Update mapping, rules, tests to absolute paths in defaults dir
            if not os.path.exists(inp.mapping):
                inp.mapping = str(defaults_dir / Path(inp.mapping).name)
            if not os.path.exists(inp.qc_rules):
                inp.qc_rules = str(defaults_dir / Path(inp.qc_rules).name)
            if not os.path.exists(inp.qc_tests):
                inp.qc_tests = str(defaults_dir / Path(inp.qc_tests).name)
        
        return config

    def _get_dashboard_config(self, key: str, default_value):
        """Get dashboard configuration value with fallback to default."""
        if self.config.app and self.config.app.dashboard:
            return getattr(self.config.app.dashboard, key, default_value)
        return default_value

    def load_data(self):
        """Load all required data files or from API."""
        if not self.config.app:
            st.error("Configuration error: 'app' section missing.")
            st.stop()

        try:
            # Load mapping configuration first as it's needed for validation
            mapping_path = self.config.app.input.mapping
            with open(mapping_path, 'r', encoding='utf-8') as f:
                self.mapping = yaml.safe_load(f)
            
            # Load processed QC results - check if API or file
            data_config = self.config.app.input.data
            
            try:
                self.data = load_data_from_config(data_config)
                # Validate the loaded data if not empty
                if not self.data.empty:
                    self.data = self._validate_api_data(self.data)
            except (DataLoadError, ConfigError) as e:
                # Show why loading failed but don't stop
                st.error(f"Data loading failed: {e}")
                # If data loading fails, we'll handle it in run() by asking
                # for upload
                self.data = pd.DataFrame()
            except Exception as e:
                st.error(f"Unexpected error loading data: {e}")
                self.data = pd.DataFrame()
            
            # Load QC rules
            rules_path = self.config.app.input.qc_rules
            self.qc_rules = pd.read_csv(rules_path, sep='\t')
            
            # Load QC tests
            tests_path = self.config.app.input.qc_tests
            self.qc_tests = pd.read_csv(tests_path, sep='\t')
            
            # Load warnings if available
            warnings_path = self.config.app.input.warnings
            if warnings_path and os.path.exists(warnings_path):
                self.warnings = pd.read_csv(warnings_path, sep='\t')
            else:
                self.warnings = None
            
        except Exception as e:
            st.error(f"Error loading configuration files: {e}")
            st.stop()

    def _get_required_fields(self) -> dict:
        """Get required fields from mapping configuration."""
        required_fields = {
            'critical': [],  # Fields that will break core functionality
            'important': []  # Fields that will break specific features
        }
        
        # Critical fields that are used throughout the application
        critical_fields = [
            'sample_name',  # Used for sample selection and display
            'qc_outcome'    # Used for metrics and filtering
        ]
        
        # Important fields that are used in specific features
        important_fields = [
            'species',           # Used in overview charts and sample details
            'provided_species',  # Used in sample details
            'qc_action'          # Used for priority coloring
        ]
        
        # Get field mappings from configuration
        sections = self.mapping.get('Sections', {})
        
        for section_data in sections.values():
            for field_config in section_data.values():
                if not isinstance(field_config, dict):
                    continue
                    
                mapping_key = field_config.get('data', {}).get('mapping')
                if mapping_key:
                    if mapping_key in critical_fields:
                        required_fields['critical'].append(mapping_key)
                    elif mapping_key in important_fields:
                        required_fields['important'].append(mapping_key)
        
        # Add fallback for critical fields if not found in mapping
        for field in critical_fields:
            if field not in required_fields['critical']:
                required_fields['critical'].append(field)
        
        return required_fields

    def _validate_api_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """Validate that API data contains required fields."""
        if data.empty:
            raise ValueError("Data source returned no data")
        
        required_fields = self._get_required_fields()
        missing_critical = []
        missing_important = []
        
        # Check for missing critical fields
        for field in required_fields['critical']:
            if field not in data.columns:
                missing_critical.append(field)
        
        # Check for missing important fields
        for field in required_fields['important']:
            if field not in data.columns:
                missing_important.append(field)
        
        # Handle missing critical fields (fatal errors)
        if missing_critical:
            raise ValueError(
                "Missing required fields: " + ", ".join(missing_critical)
            )
        
        # Handle missing important fields (warnings)
        if missing_important:
            st.warning(
                "Optional fields missing: " + ", ".join(missing_important)
            )
        
        return data


    def setup_page(self):
        """Set up the Streamlit page configuration."""
        st.set_page_config(
            page_title=self.config.title,
            page_icon="üî¨",
            layout="wide",
            initial_sidebar_state="expanded"
        )

    def render_header(self):
        """Render the application header."""
        st.title("üî¨ uQCme - Microbial Quality Control Dashboard")
    
    def render_sidebar_metrics(self, filtered_data: pd.DataFrame):
        """Render summary metrics in the sidebar."""
        st.sidebar.subheader("üìä Summary")
        
        # Version info
        version = self.config.version
        st.sidebar.markdown(f"**Version:** {version}")
        
        # Create columns for horizontal layout
        col1, col2, col3 = st.sidebar.columns(3)
        
        # Sample count metrics (filtered vs total)
        total_samples = len(filtered_data)
        total_all = len(self.data)
        with col1:
            # Show filtered count vs total
            if total_samples == total_all:
                st.metric("Samples", total_samples)
            else:
                delta_text = f"of {total_all}"
                st.metric("Samples", total_samples, delta=delta_text)
        
        st.sidebar.markdown("---")

        # QC outcome metrics (based on filtered data)
        pass_filter = filtered_data['qc_outcome'] == 'PASS'
        pass_count = len(filtered_data[pass_filter])
        
        # Calculate total PASS from unfiltered data
        total_pass_filter = self.data['qc_outcome'] == 'PASS'
        total_pass_count = len(self.data[total_pass_filter])
        
        with col2:
            if pass_count == total_pass_count:
                st.metric("PASS", pass_count)
            else:
                delta_text = f"of {total_pass_count}"
                st.metric("PASS", pass_count, delta=delta_text)
        
        fail_filter = filtered_data['qc_outcome'] != 'PASS'
        fail_count = len(filtered_data[fail_filter])
        
        # Calculate total Issues from unfiltered data
        total_fail_filter = self.data['qc_outcome'] != 'PASS'
        total_fail_count = len(self.data[total_fail_filter])
        
        with col3:
            if fail_count == total_fail_count:
                st.metric("Issues", fail_count)
            else:
                delta_text = f"of {total_fail_count}"
                st.metric("Issues", fail_count, delta=delta_text)

    def _get_filterable_fields(self, data: pd.DataFrame) -> list:
        """Get all fields that should have filters based on mapping config."""
        filterable_fields = []
        sections_columns = self._get_columns_by_section(data)
        
        for section_name, section_cols in sections_columns.items():
            for col_info in section_cols:
                if col_info['filter'] and col_info['column'] in data.columns:
                    filterable_fields.append({
                        'column': col_info['column'],
                        'field_name': col_info['field_name'],
                        'section': section_name
                    })
        
        return filterable_fields

    def _create_numerical_filter(self, filtered_data: pd.DataFrame,
                                 column: str, field_name: str) -> pd.DataFrame:
        """Create and apply numerical range filter."""
        unique_values = filtered_data[column].dropna()
        
        if len(unique_values) == 0:
            return filtered_data
            
        min_val = float(unique_values.min())
        max_val = float(unique_values.max())
        
        # Only show slider if there's a range
        if min_val == max_val:
            return filtered_data
        
        # Check if filters should be reset
        reset_filters = st.session_state.get('filters_reset', False)
        default_value = (min_val, max_val)
        
        # Clear the reset flag after using it
        if reset_filters:
            key = f"range_{column}"
            if key in st.session_state:
                del st.session_state[key]
            
        selected_range = st.sidebar.slider(
            f"{field_name} Range",
            min_value=min_val,
            max_value=max_val,
            value=default_value,
            key=f"range_{column}"
        )
        
        # Apply range filter
        # Include NaN values when slider is at full range
        full_range = (selected_range[0] == min_val and
                      selected_range[1] == max_val)
        if full_range:
            # Full range selected - don't filter anything
            return filtered_data
        else:
            # Partial range - apply filter but preserve NaN
            col_data = filtered_data[column]
            min_check = col_data >= selected_range[0]
            max_check = col_data <= selected_range[1]
            in_range = min_check & max_check
            range_condition = (in_range | col_data.isna())
            return filtered_data[range_condition]

    def _create_categorical_filter(self, filtered_data: pd.DataFrame,
                                   column: str,
                                   field_name: str) -> pd.DataFrame:
        """Create and apply categorical dropdown filter."""
        unique_values = filtered_data[column].dropna()
        
        if len(unique_values) == 0:
            return filtered_data
            
        unique_sorted = sorted(unique_values.unique())
        
        # Only create filter if we have reasonable number of options
        threshold = self._get_dashboard_config(
            'categorical_filter_threshold', 20
        )
        if len(unique_sorted) > threshold:
            return filtered_data
            
        options = ['All'] + list(unique_sorted)
        
        # Check if filters should be reset
        reset_filters = st.session_state.get('filters_reset', False)
        default_index = 0  # 'All'
        
        # Clear the reset flag after using it
        if reset_filters:
            key = f"filter_{column}"
            if key in st.session_state:
                del st.session_state[key]
        
        selected_value = st.sidebar.selectbox(
            f"Filter by {field_name}",
            options,
            index=default_index,
            key=f"filter_{column}"
        )
        
        if selected_value != 'All':
            filter_condition = (filtered_data[column] == selected_value)
            return filtered_data[filter_condition]
        
        return filtered_data

    def _create_text_search_filter(self, filtered_data: pd.DataFrame,
                                   column: str,
                                   field_name: str) -> pd.DataFrame:
        """Create and apply text search filter."""
        # Check if filters should be reset
        reset_filters = st.session_state.get('filters_reset', False)
        default_value = ""
        
        # Clear the reset flag after using it
        if reset_filters:
            key = f"search_{column}"
            if key in st.session_state:
                del st.session_state[key]
        
        search_value = st.sidebar.text_input(
            f"Search {field_name}",
            placeholder=f"Enter {field_name.lower()}...",
            value=default_value,
            key=f"search_{column}"
        )
        
        if search_value:
            column_str = filtered_data[column].astype(str)
            contains_filter = column_str.str.contains(
                search_value, case=False, na=False
            )
            return filtered_data[contains_filter]
        
        return filtered_data

    def _clear_all_filters(self):
        """Clear all filter-related session state values and selections."""
        # Set a reset flag instead of trying to modify widget values directly
        st.session_state['filters_reset'] = True
        
        # Clear sample selections
        if 'selected_samples' in st.session_state:
            st.session_state.selected_samples.clear()
        
        # Force a rerun to refresh the interface
        st.rerun()

    def render_sidebar_filters(self):
        """Render sidebar filters for data exploration."""
        # Get filterable fields from mapping configuration
        filterable_fields = self._get_filterable_fields(self.data)
        
        # Apply filters
        filtered_data = self.data.copy()
        
        # Generate dynamic filters based on mapping configuration
        for field_info in filterable_fields:
            column = field_info['column']
            field_name = field_info['field_name']
            
            if column in filtered_data.columns:
                # Get unique values for this column
                unique_values = filtered_data[column].dropna()
                
                if len(unique_values) > 0:
                    # Check if column is numerical
                    is_numeric = pd.api.types.is_numeric_dtype(unique_values)
                    
                    if is_numeric:
                        # Use extracted numerical filter method
                        filtered_data = self._create_numerical_filter(
                            filtered_data, column, field_name
                        )
                    else:
                        # Categorical or text filters for non-numerical columns
                        unique_sorted = sorted(unique_values.unique())
                        
                        # Determine filter type based on unique count
                        threshold = self._get_dashboard_config(
                            'categorical_filter_threshold', 20
                        )
                        if len(unique_sorted) <= threshold:
                            # Use extracted categorical filter method
                            filtered_data = self._create_categorical_filter(
                                filtered_data, column, field_name
                            )
                        else:
                            # Use extracted text search filter method
                            filtered_data = self._create_text_search_filter(
                                filtered_data, column, field_name
                            )
        
        # Add sample name search (always available)
        # Check if filters should be reset
        reset_filters = st.session_state.get('filters_reset', False)
        default_sample_value = ""
        
        # Clear the reset flag for sample search
        if reset_filters:
            key = "search_sample_name"
            if key in st.session_state:
                del st.session_state[key]
        
        sample_filter = st.sidebar.text_input(
            "Search Sample Names",
            placeholder="Enter sample name...",
            value=default_sample_value,
            key="search_sample_name"
        )
        
        if sample_filter:
            contains_filter = filtered_data['sample_name'].str.contains(
                sample_filter, case=False, na=False
            )
            filtered_data = filtered_data[contains_filter]
        
        # Clear the reset flag after all filters have been processed
        if st.session_state.get('filters_reset', False):
            st.session_state['filters_reset'] = False
        
        # Render summary metrics at the top with filtered data
        self.render_sidebar_metrics(filtered_data)
        
        st.sidebar.header("üîç Filters")
        
        # Add Clear All Filters button
        if st.sidebar.button("üóëÔ∏è Clear All Filters", type="secondary"):
            self._clear_all_filters()
        
        return filtered_data

    def _get_columns_by_section(self, data: pd.DataFrame) -> Dict[str, list]:
        """Get columns organized by section from mapping.yaml."""
        sections_columns = {}
        
        # Get sections from mapping
        sections = self.mapping.get('Sections', {})
        
        for section_name, section_data in sections.items():
            section_cols = []
            
            for field_name, field_config in section_data.items():
                # Skip if field_config is not a dict (e.g., boolean values)
                if not isinstance(field_config, dict):
                    continue
                    
                mapping_key = field_config.get('data', {}).get('mapping')
                if mapping_key and mapping_key in data.columns:
                    # Get report configuration
                    report_config = field_config.get('report', {})
                    
                    # Include hidden fields in the section but mark them
                    is_hidden = (field_config.get('hidden', False) or
                                 report_config.get('hidden', False))
                    
                    section_cols.append({
                        'column': mapping_key,
                        'field_name': field_name,
                        'hidden': is_hidden,
                        'filter': report_config.get('filter', False),
                        'id': report_config.get('id', False)
                    })
            
            if section_cols:  # Only add sections that have columns
                sections_columns[section_name] = section_cols
        
        # Add unmapped columns to "Other" section
        all_mapped_cols = []
        for section_cols in sections_columns.values():
            all_mapped_cols.extend([col['column'] for col in section_cols])
        
        unmapped_cols = [
            col for col in data.columns
            if col not in all_mapped_cols
        ]
        
        if unmapped_cols:
            sections_columns['Other'] = [
                {
                    'column': col,
                    'field_name': col.replace('_', ' ').title(),
                    'hidden': False,
                    'filter': False,
                    'id': False
                }
                for col in unmapped_cols
            ]
        
        return sections_columns

    def _get_id_column(self, data: pd.DataFrame) -> Optional[str]:
        """Get the column marked as ID field in mapping configuration."""
        sections_columns = self._get_columns_by_section(data)
        
        for section_cols in sections_columns.values():
            for col_info in section_cols:
                if col_info.get('id', False):
                    return col_info['column']
        
        return None

    def _get_column_description(self, column_name: str) -> Optional[str]:
        """Get description for a column from mapping configuration."""
        sections = self.mapping.get('Sections', {})
        
        for section_data in sections.values():
            for field_name, field_config in section_data.items():
                # Skip if field_config is not a dict
                if not isinstance(field_config, dict):
                    continue
                    
                mapping_key = field_config.get('data', {}).get('mapping')
                if mapping_key == column_name:
                    # Check for description in report config first
                    report_config = field_config.get('report', {})
                    description = report_config.get('description')
                    if description:
                        return description
                    
                    # Fallback: check QC config for backward compatibility
                    qc_config = field_config.get('QC', {})
                    description = qc_config.get('description')
                    if description:
                        return description
                    
                    # Fallback to field name if no description
                    return field_name
        
        # Return None if no mapping found
        return None

    def _get_priority_colored_columns(self, data: pd.DataFrame) -> list:
        """Get columns that should have priority coloring based on mapping."""
        priority_columns = []
        sections_columns = self._get_columns_by_section(data)
        
        for section_cols in sections_columns.values():
            for col_info in section_cols:
                column_name = col_info['column']
                # Check if this column has priority_coloring enabled
                if (column_name in data.columns and
                        self._column_has_priority_coloring(column_name)):
                    priority_columns.append(column_name)
        
        return priority_columns
    
    def _column_has_priority_coloring(self, column_name: str) -> bool:
        """Check if a column has priority coloring enabled in mapping."""
        sections = self.mapping.get('Sections', {})
        
        for section_data in sections.values():
            for field_config in section_data.values():
                if not isinstance(field_config, dict):
                    continue
                    
                mapping_key = field_config.get('data', {}).get('mapping')
                if mapping_key == column_name:
                    report_config = field_config.get('report', {})
                    return report_config.get('priority_coloring', False)
        
        return False

    def _get_ordered_columns_with_sections(
        self,
        data: pd.DataFrame,
        visible_sections: Dict[str, bool]
    ) -> list:
        """Get ordered columns based on visible sections."""
        ordered_cols = []
        sections_columns = self._get_columns_by_section(data)
        
        # Define section order
        section_order = ['Basic', 'QC_metrics', 'Experimental', 'Other']
        
        # Add columns from visible sections in order
        for section_name in section_order:
            section_visible = visible_sections.get(section_name, True)
            if section_name in sections_columns and section_visible:
                for col_info in sections_columns[section_name]:
                    # Skip hidden fields
                    if col_info['hidden']:
                        continue
                    ordered_cols.append(col_info['column'])
        
        # Add remaining sections not in the predefined order
        for section_name, section_cols in sections_columns.items():
            section_visible = visible_sections.get(section_name, True)
            section_not_ordered = section_name not in section_order
            if section_not_ordered and section_visible:
                for col_info in section_cols:
                    if col_info['column'] not in ordered_cols:
                        ordered_cols.append(col_info['column'])
        
        return ordered_cols

    def _get_qc_outcome_priority(self, outcome: str) -> int:
        """Get priority level for QC outcome."""
        # Get outcome priority mapping from config, with fallback defaults
        config_priorities = self.config.outcome_priorities or {}
        
        # Default outcome priority mapping (higher number = higher priority)
        default_priorities = {
            'PASS': 1,
            'WARNING': 2,
            'FAIL': 3,
            'ERROR': 4
        }
        
        # Use config priority if available, otherwise use default
        outcome_upper = outcome.upper()
        if outcome_upper in config_priorities:
            return config_priorities[outcome_upper]
        else:
            return default_priorities.get(outcome_upper, 4)

    def _get_qc_outcome_color(self, outcome: str) -> str:
        """Get color for QC outcome based on priority."""
        priority = self._get_qc_outcome_priority(outcome)
        priority_colors = {}
        if self.config.app and self.config.app.priority_colors:
            priority_colors = self.config.app.priority_colors
        
        # Default color mapping as fallback (darker text-friendly colors)
        default_color_mapping = {
            1: "#00AA00",  # Dark green for PASS
            2: "#FF8C00",  # Dark orange for WARNING
            3: "#DC143C",  # Dark red for FAIL
            4: "#8B0000"   # Dark red for ERROR
        }
        
        # Use config color if available, otherwise use default mapping
        if priority in priority_colors:
            return priority_colors[priority]
        else:
            return default_color_mapping.get(priority, "#000000")

    def _get_qc_action_color(self, action: str) -> str:
        """Get color for QC action based on action type."""
        # Map actions to colors
        action_colors = {
            'none': "#00AA00",      # Green for no action needed
            'review': "#FF8C00",    # Orange for review needed
            'reject': "#DC143C",    # Red for reject
            'return_to_lab': "#8B0000"  # Dark red for return to lab
        }
        
        action_lower = action.lower()
        return action_colors.get(action_lower, "#000000")

    def _render_plotly_chart(self, fig, key: str, title: Optional[str] = None):
        """Helper method to render plotly charts with consistent styling."""
        if title:
            st.subheader(title)
        if fig:
            st.plotly_chart(
                fig,
                width='content',
                key=key
            )

    def _render_styled_dataframe(self, filtered_data: pd.DataFrame,
                                 column_order: list, key: str):
        """Helper method to render dataframe with QC styling and selection."""
        # Initialize session state for selected samples
        if 'selected_samples' not in st.session_state:
            st.session_state.selected_samples = set()
        
        # Get the ID column for sample selection
        id_column = self._get_id_column(filtered_data)
        
        # Create a working copy of the data
        display_data = filtered_data.copy()
        
        # Add selection checkbox column if ID column exists
        if id_column and id_column in filtered_data.columns:
            # Add a checkbox column for selection
            display_data['Select'] = display_data[id_column].apply(
                lambda x: x in st.session_state.selected_samples
            )
            # Put the select column first
            column_order = ['Select'] + column_order
        
        # Configure columns
        column_config = {}
        
        # Configure the select column if present
        if 'Select' in display_data.columns:
            column_config['Select'] = st.column_config.CheckboxColumn(
                "Select",
                help="Select this sample"
            )
        
        # Apply QC styling for columns with priority coloring enabled
        priority_columns = self._get_priority_colored_columns(display_data)
        
        if priority_columns:
            def highlight_priority_values(val, column_name):
                if pd.isna(val):
                    return ''
                
                # For qc_outcome columns, use outcome-based coloring
                if column_name == 'qc_outcome':
                    color = self._get_qc_outcome_color(str(val))
                # For qc_action columns, use action-based coloring
                elif column_name == 'qc_action':
                    color = self._get_qc_action_color(str(val))
                else:
                    # For other priority columns, try to get color by value
                    color = self._get_qc_outcome_color(str(val))
                
                return (f'color: {color}; font-weight: bold; '
                        f'text-shadow: 0 0 3px {color};')
            
            # Apply styling to all priority columns at once
            styled_data = display_data.style
            for column in priority_columns:
                if column in display_data.columns:
                    styled_data = styled_data.map(
                        lambda val, col=column: highlight_priority_values(
                            val, col),
                        subset=[column]
                    )
        else:
            styled_data = display_data
        
        # Disable all columns except Select to prevent accidental editing
        disabled_columns = [col for col in display_data.columns
                            if col != 'Select']
        
        # Use data_editor to enable checkbox interaction
        edited_data = st.data_editor(
            styled_data,
            width='stretch',
            key=key,
            column_order=column_order,
            column_config=column_config,
            hide_index=True,
            disabled=disabled_columns
        )
        
        # Update selected samples based on checkbox changes
        if 'Select' in edited_data.columns and id_column:
            # Get current selections from the edited data
            current_selections = set()
            for idx, row in edited_data.iterrows():
                if row['Select']:
                    sample_id = str(row[id_column])
                    current_selections.add(sample_id)
            
            # Update session state if there are changes
            if current_selections != st.session_state.selected_samples:
                st.session_state.selected_samples = current_selections
                st.rerun()
        
        # Display selected samples summary
        if st.session_state.selected_samples:
            selected_samples = sorted(st.session_state.selected_samples)
            selected_text = ', '.join(selected_samples)
            st.info(f"**Selected samples:** {selected_text}")
            
            if st.button("Clear Selection", key=f"clear_{key}"):
                st.session_state.selected_samples.clear()
                st.rerun()

    def render_data_tab(self, filtered_data: pd.DataFrame):
        """Render the data tab with section toggles."""
        st.header("üìä Data")
        
        # Section toggles
        st.subheader("Section Visibility")
        sections_columns = self._get_columns_by_section(filtered_data)
        
        # Create columns for section toggles
        num_columns = self._get_dashboard_config('section_toggle_columns', 3)
        toggle_columns = st.columns(num_columns)
        
        visible_sections = {}
        section_names = list(sections_columns.keys())
        
        # Distribute toggles across columns
        for i, section_name in enumerate(section_names):
            col_idx = i % num_columns
            
            with toggle_columns[col_idx]:
                # Count visible columns in section
                section_cols = sections_columns[section_name]
                visible_col_count = len([
                    col for col in section_cols
                    if not col['hidden']
                ])
                
                # Default visibility based on content
                # Hide sections that are mostly hidden fields or experimental
                default_visible = True
                if visible_col_count == 0:
                    default_visible = False
                
                visible_sections[section_name] = st.checkbox(
                    f"{section_name} ({visible_col_count} columns)",
                    value=default_visible,
                    key=f"data_preview_toggle_{section_name}"
                )
        
        # Get ordered columns based on visible sections for reference
        ordered_columns = self._get_ordered_columns_with_sections(
            filtered_data,
            visible_sections
        )
        
        # Show active sections info
        active_sections = [
            name for name, visible in visible_sections.items()
            if visible
        ]
        
        # Display helpful info about column organization
        if ordered_columns:
            tip_msg = (
                f"üí° **Tip:** Use the column visibility controls (üëÅÔ∏è) in the "
                f"table to show/hide specific columns. Currently showing "
                f"{len(ordered_columns)} columns from selected sections: "
                f"{', '.join(active_sections)}"
            )
            st.info(tip_msg)
        
        # Reorder dataframe columns to put important ones first
        # while preserving all columns for eyeball functionality
        priority_columns = []
        other_columns = []
        
        # Add columns in section order priority
        for col in ordered_columns:
            if col in filtered_data.columns:
                priority_columns.append(col)
        
        # Add remaining columns
        for col in filtered_data.columns:
            if col not in priority_columns:
                other_columns.append(col)
        
        # Create column order for Streamlit's column_order parameter
        column_order = priority_columns + other_columns
        
        # Display the dataframe with built-in controls and QC outcome styling
        # Use the full filtered data to ensure column controls are available
        self._render_styled_dataframe(
            filtered_data, column_order, "data_preview_table"
        )
        
        # Show column information organized by visible sections
        with st.expander("üìã Column Information"):
            st.write("**Column mapping from configuration:**")
            
            for section_name in active_sections:
                if section_name in sections_columns:
                    st.subheader(f"{section_name} Section")
                    section_cols = sections_columns[section_name]
                    
                    for col_info in section_cols:
                        mapping_key = col_info['column']
                        field_name = col_info['field_name']
                        hidden = col_info['hidden']
                        has_filter = col_info['filter']
                        is_id = col_info['id']
                        
                        if mapping_key in filtered_data.columns:
                            # Skip hidden fields from display
                            if hidden:
                                continue
                            
                            # Get description from mapping config
                            description = self._get_column_description(
                                mapping_key
                            )
                            
                            # Build display string with additional info
                            extras = []
                            if has_filter:
                                extras.append("Filterable")
                            if is_id:
                                extras.append("ID")
                            
                            extra_info = ""
                            if extras:
                                extra_info = f" ({', '.join(extras)})"
                            
                            # Create field display with description
                            if description and description != field_name:
                                field_display = (
                                    f"- **{field_name}**: `{mapping_key}` - "
                                    f"{description}{extra_info}"
                                )
                            else:
                                field_display = (
                                    f"- **{field_name}**: `{mapping_key}`"
                                    f"{extra_info}"
                                )
                            st.write(field_display)

    def render_overview_tab(self, filtered_data: pd.DataFrame):
        """Render the overview tab with summary statistics."""
        st.header("üìà Overview")
        
        # Use the plotter to create overview charts
        overview_plots = self.plotter.create_quality_overview_dashboard(
            filtered_data
        )
        
        # Display charts in columns
        col1, col2 = st.columns(2)
        
        with col1:
            self._render_plotly_chart(
                overview_plots.get('outcome_pie'),
                "overview_outcome_pie",
                "QC Outcomes Distribution"
            )
        
        with col2:
            self._render_plotly_chart(
                overview_plots.get('species_bar'),
                "overview_species_bar",
                "Species Distribution"
            )
        
        # Failed Rules Analysis
        self._render_plotly_chart(
            overview_plots.get('failed_rules'),
            "overview_failed_rules",
            "Most Common Failed Rules"
        )
        
        # Quality metrics if available
        self._render_plotly_chart(
            overview_plots.get('metric_dist'),
            "overview_metric_dist",
            "Quality Metrics Distribution"
        )
        
        self._render_plotly_chart(
            overview_plots.get('correlation'),
            "overview_correlation",
            "Metrics Correlation"
        )

    def render_quality_metrics_tab(self, filtered_data: pd.DataFrame):
        """Render quality metrics visualizations."""
        st.header("üîç Quality Metrics")
        
        # Get available numeric columns
        available_cols = get_available_metrics(filtered_data)
        
        if not available_cols:
            warning_msg = (
                "No numeric quality metrics available for visualization."
            )
            st.warning(warning_msg)
            return
        
        # Metrics selection
        col1, col2 = st.columns(2)
        
        with col1:
            selected_metric = st.selectbox(
                "Select Quality Metric",
                available_cols,
                format_func=lambda x: self.plotter._format_column_name(x),
                index=0
            )
        
        with col2:
            chart_type = st.selectbox(
                "Chart Type",
                ["Distribution", "Box Plot", "Scatter Plot"],
                index=0
            )
        
        # Create visualizations using plotter
        if selected_metric and chart_type:
            if chart_type == "Distribution":
                fig = self.plotter.create_distribution_plot(
                    filtered_data, selected_metric
                )
                self._render_plotly_chart(fig, "metrics_distribution")
                
            elif chart_type == "Box Plot":
                fig = self.plotter.create_box_plot(
                    filtered_data, selected_metric
                )
                self._render_plotly_chart(fig, "metrics_box_plot")
                
            elif chart_type == "Scatter Plot":
                # Find another metric for comparison
                other_metrics = [
                    col for col in available_cols
                    if col != selected_metric
                ]
                
                if other_metrics:
                    def format_metric_name(x):
                        return self.plotter._format_column_name(x)
                    
                    y_metric = st.selectbox(
                        "Select Y-axis metric",
                        other_metrics,
                        format_func=format_metric_name,
                        index=0
                    )
                    
                    if y_metric:
                        fig = self.plotter.create_scatter_plot(
                            filtered_data, selected_metric, y_metric
                        )
                        self._render_plotly_chart(fig, "metrics_scatter_plot")
                else:
                    warning_msg = (
                        "No additional metrics available for scatter plot."
                    )
                    st.warning(warning_msg)

    def render_sample_details_tab(self, filtered_data: pd.DataFrame):
        """Render detailed sample information."""
        st.header("üî¨ Sample Details")
        
        # Sample selection
        sample_options = filtered_data['sample_name'].tolist()
        
        if not sample_options:
            st.warning("No samples match the current filters.")
            return
        
        selected_sample = st.selectbox(
            "Select Sample (based on filtered data)",
            sample_options
        )
        
        # Get sample data
        selected_filter = filtered_data['sample_name'] == selected_sample
        sample_data = filtered_data[selected_filter].iloc[0]
        
        # Display sample information
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Basic Information")
            st.write(f"**Sample Name:** {sample_data['sample_name']}")
            st.write(f"**Species:** {sample_data['species']}")
            provided_spec = sample_data.get('provided_species', 'N/A')
            st.write(f"**Provided Species:** {provided_spec}")
            st.write(f"**QC Outcome:** {sample_data['qc_outcome']}")
        
        with col2:
            st.subheader("Quality Metrics")
            if 'GC' in sample_data:
                st.write(f"**GC Content:** {sample_data['GC']:.2f}%")
            if 'N50' in sample_data:
                st.write(f"**N50:** {sample_data['N50']:,.0f}")
            if 'bin_length_at_1x' in sample_data:
                genome_size = sample_data['bin_length_at_1x']
                st.write(f"**Genome Size:** {genome_size:,.0f} bp")
        
        # Failed and passed rules
        st.subheader("QC Rules Analysis")
        
        col1, col2 = st.columns(2)
        
        with col1:
            failed_rules_val = sample_data.get('failed_rules')
            if (failed_rules_val and
                    pd.notna(failed_rules_val) and
                    isinstance(failed_rules_val, str)):
                st.write("**Failed Rules:**")
                failed_rules = failed_rules_val.split(',')
                # Display all failed rules, each on one row
                st.write("‚ùå " + ", ".join([rule.strip() for rule in failed_rules]))
            else:
                st.write("‚úÖ No failed rules")
        
        with col2:
            passed_rules_val = sample_data.get('passed_rules')
            if (passed_rules_val and
                    pd.notna(passed_rules_val) and
                    isinstance(passed_rules_val, str)):
                st.write("**Passed Rules:**")
                passed_rules = passed_rules_val.split(',')
                # Display all passed rules, all on one row
                st.write("‚úÖ " + ", ".join([rule.strip() for rule in passed_rules]))
            else:
                st.write("No passed rules data available")

    def render_qc_tests_tab(self):
        """Render the QC tests configuration tab."""
        st.header("‚öôÔ∏è QC Tests Configuration")
        
        if self.qc_tests.empty:
            st.warning("No QC tests data available.")
            return
        
        # Display QC tests overview
        st.subheader("Available QC Tests")
        st.write(f"**Total QC tests configured:** {len(self.qc_tests)}")
        
        # QC tests selection
        st.subheader("Select QC Test")
        
        # Prepare QC tests dataframe for selection
        display_tests = self.qc_tests.copy()
        
        # Add formatted display columns
        display_tests['Priority_Label'] = display_tests['priority'].astype(str)
        display_tests['Test_Name'] = display_tests.apply(
            lambda row: row.get('outcome_name', row['outcome_id']), axis=1
        )
        
        # Select columns for display
        display_columns = [
            'outcome_id', 'Test_Name', 'Priority_Label',
            'action_required', 'description'
        ]
        
        # Ensure all columns exist
        for col in display_columns:
            if col not in display_tests.columns:
                display_tests[col] = 'N/A'
        
        # Sort by priority (highest first)
        display_tests = display_tests.sort_values('priority', ascending=False)
        
        # Create display dataframe
        selection_df = display_tests[display_columns].copy()
        selection_df.columns = [
            'Outcome ID', 'Test Name', 'Priority', 'Action Required',
            'Description'
        ]
        
        # Display selectable dataframe
        selected_rows = st.dataframe(
            selection_df,
            width='stretch',
            hide_index=True,
            on_select="rerun",
            selection_mode="single-row",
            key="qc_tests_selection"
        )
        
        # Get selected test
        if selected_rows['selection']['rows']:
            selected_idx = selected_rows['selection']['rows'][0]
            selected_test = display_tests.iloc[selected_idx]
        else:
            # Default to first test if none selected
            selected_test = display_tests.iloc[0]
            st.info("üëÜ Select a QC test from the table above to view details.")
        
        # Display test details
        st.subheader("Test Details")
        st.write(f"**Outcome ID:** {selected_test['outcome_id']}")
        outcome_name = selected_test.get('outcome_name', 'N/A')
        st.write(f"**Name:** {outcome_name}")
        st.write(f"**Priority:** {selected_test['priority']}")
        description = selected_test.get('description', 'N/A')
        st.write(f"**Description:** {description}")
        action = selected_test['action_required']
        st.write(f"**Action Required:** {action}")
        conditions = selected_test['rule_conditions']
        st.write(f"**Rule Conditions:** {conditions}")
        
        # Related QC Rules Table
        st.subheader("Related QC Rules")
        
        if self.qc_rules.empty:
            st.warning("No QC rules data available.")
            return
        
        # Parse rule conditions to find related rules
        conditions = selected_test['rule_conditions']
        
        if pd.notna(conditions) and isinstance(conditions, str):
            if conditions == 'no_failed_rules':
                st.info("This test passes when no rules fail. "
                        "Showing all QC rules:")
                # Show all rules in a table
                rules_table_data = []
                for _, rule in self.qc_rules.iterrows():
                    rules_table_data.append({
                        'Rule ID': rule.get('rule_id', 'Unknown'),
                        'Species': rule.get('species', 'N/A'),
                        'Assembly Type': rule.get('assembly_type', 'N/A'),
                        'Software': rule.get('software', 'N/A'),
                        'Field': rule.get('field', 'N/A'),
                        'Operator': rule.get('operator', 'N/A'),
                        'Value': rule.get('value', 'N/A'),
                        'Special Field': rule.get('special_field', 'N/A')
                    })
                
                rules_df = pd.DataFrame(rules_table_data)
                st.dataframe(
                    rules_df,
                    width='stretch',
                    hide_index=True
                )
                
            elif conditions.startswith('failed_rules_contain:'):
                # Extract rule IDs and show them in a table
                condition_part = conditions.replace(
                    'failed_rules_contain:', ''
                )
                rule_ids = [
                    rule.strip() for rule in condition_part.split(',')
                ]
                
                num_rules = len(rule_ids)
                st.info(f"This test triggers when any of these "
                        f"{num_rules} rules fail:")
                
                # Create table with matching rules
                rules_table_data = []
                for rule_id in rule_ids:
                    matching_rules = self.qc_rules[
                        self.qc_rules['rule_id'] == rule_id
                    ]
                    
                    if not matching_rules.empty:
                        rule = matching_rules.iloc[0]
                        rules_table_data.append({
                            'Rule ID': rule_id,
                            'Species': rule.get('species', 'N/A'),
                            'Assembly Type': rule.get('assembly_type', 'N/A'),
                            'Software': rule.get('software', 'N/A'),
                            'Field': rule.get('field', 'N/A'),
                            'Operator': rule.get('operator', 'N/A'),
                            'Value': rule.get('value', 'N/A'),
                            'Special Field': rule.get('special_field', 'N/A')
                        })
                    else:
                        rules_table_data.append({
                            'Rule ID': rule_id,
                            'Species': 'Rule not found',
                            'Assembly Type': '-',
                            'Software': '-',
                            'Field': '-',
                            'Operator': '-',
                            'Value': '-',
                            'Special Field': '-'
                        })
                
                if rules_table_data:
                    rules_df = pd.DataFrame(rules_table_data)
                    st.dataframe(
                        rules_df,
                        width='stretch',
                        hide_index=True
                    )
                else:
                    st.warning("No matching rules found.")
            else:
                st.warning("Rule condition format not recognized.")
                st.code(conditions)
        else:
            st.warning("No rule conditions specified for this test.")

    def render_warnings_tab(self):
        """Render the warnings tab showing processing warnings and issues."""
        st.header("‚ö†Ô∏è Processing Warnings")
        
        if self.warnings is None or self.warnings.empty:
            st.info("No warnings file found or no warnings generated "
                    "during processing.")
            
            # Show information about warnings output
            warnings_path = (
                self.config.app.input.warnings if self.config.app else None
            )
            if warnings_path:
                st.write(f"**Expected warnings file:** `{warnings_path}`")
                st.write("Warnings will be saved here during the next "
                         "uQCme processing run.")
            else:
                st.write("No warnings input path configured in config.yaml")
            return
        
        # Display summary statistics
        st.subheader("üìä Warnings Summary")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            total_warnings = len(self.warnings)
            st.metric("Total Warnings", total_warnings)
            # Add a divider
            st.markdown("---")

        with col2:
            warning_types = self.warnings['warning_type'].nunique()
            st.metric("Warning Types", warning_types)
        
        with col3:
            # Get most recent warning timestamp if available
            if 'timestamp' in self.warnings.columns:
                latest_warning = self.warnings['timestamp'].max()
                formatted_date = (
                    latest_warning.split('T')[0]
                    if 'T' in str(latest_warning)
                    else str(latest_warning)
                )
                st.metric("Latest Warning", formatted_date)
        
        # Filter by warning type
        st.subheader("üîç Filter Warnings")
        
        col1, col2 = st.columns(2)
        
        with col1:
            warning_types = (
                ['All'] +
                sorted(self.warnings['warning_type'].unique().tolist())
            )
            selected_type = st.selectbox(
                "Warning Type",
                warning_types,
                key="warnings_type_filter"
            )
        
        with col2:
            # Search in warning messages
            search_term = st.text_input(
                "Search in messages",
                placeholder="Enter search term...",
                key="warnings_search"
            )
        
        # Apply filters
        filtered_warnings = self.warnings.copy()
        
        if selected_type != 'All':
            filtered_warnings = filtered_warnings[
                filtered_warnings['warning_type'] == selected_type
            ]
        
        if search_term:
            filtered_warnings = filtered_warnings[
                filtered_warnings['warning_message'].str.contains(
                    search_term, case=False, na=False
                )
            ]
        
        # Display warnings table
        st.subheader("üìã Warnings Details")
        
        if filtered_warnings.empty:
            st.info("No warnings match the current filters.")
        else:
            st.write(f"Showing {len(filtered_warnings)} of "
                     f"{len(self.warnings)} warnings")
            
            # Format warnings for display
            display_warnings = filtered_warnings.copy()
            
            # Format timestamp if available
            if 'timestamp' in display_warnings.columns:
                display_warnings['timestamp'] = (
                    pd.to_datetime(display_warnings['timestamp'])
                    .dt.strftime('%Y-%m-%d %H:%M:%S')
                )
            
            # Rename columns for better display
            column_mapping = {
                'warning_type': 'Type',
                'warning_message': 'Message',
                'timestamp': 'Timestamp'
            }
            
            display_warnings = display_warnings.rename(columns=column_mapping)
            
            # Display as a dataframe with styling
            st.dataframe(
                display_warnings,
                width='stretch',
                hide_index=True,
                column_config={
                    'Type': st.column_config.TextColumn(
                        'Type', width="small"
                    ),
                    'Message': st.column_config.TextColumn(
                        'Message', width="large"
                    ),
                    'Timestamp': st.column_config.TextColumn(
                        'Timestamp', width="medium"
                    )
                }
            )
            
            # Show warning type breakdown
            if len(filtered_warnings) > 1:
                st.subheader("üìà Warning Type Distribution")
                warning_counts = (
                    filtered_warnings['warning_type'].value_counts()
                )
                
                for warning_type, count in warning_counts.items():
                    percentage = (count / len(filtered_warnings)) * 100
                    st.write(f"**{warning_type}:** {count} warnings "
                             f"({percentage:.1f}%)")

    def _process_uploaded_file(self, uploaded_file) -> bool:
        """Process an uploaded raw data file and update self.data."""
        try:
            with st.spinner("Processing data..."):
                # Try to read as TSV first (standard for this tool)
                uploaded_file.seek(0)
                try:
                    df = pd.read_csv(uploaded_file, sep='\t')
                except Exception:
                    df = pd.DataFrame()

                # If it looks like it failed to parse (only 1 column), try CSV
                if len(df.columns) <= 1:
                    uploaded_file.seek(0)
                    try:
                        df_csv = pd.read_csv(uploaded_file, sep=',')
                        if len(df_csv.columns) > 1:
                            df = df_csv
                    except Exception:
                        # If CSV fails, stick with the TSV result
                        pass
                
                if df.empty:
                    st.error("Uploaded file is empty or could not be read.")
                    return False

                # Check for ID column
                id_col = self._get_id_column(df)
                if not id_col and 'sample_name' not in df.columns:
                    st.error(
                        "‚ö†Ô∏è Could not find the configured ID column or "
                        "'sample_name' in the uploaded file. Please check "
                        "your file format (TSV/CSV) and headers."
                    )
                    return False

                # Initialize processor
                processor = QCProcessor(self.config_path)
                
                # Load reference data
                processor.load_reference_data()
                
                # Set run data directly
                processor.run_data = df
                
                # Process
                processor.process_samples()
                
                # Get results
                self.data = processor.results
                
                # Convert warnings set to DataFrame for display
                warnings_data = []
                for warning in sorted(processor.warnings):
                    warnings_data.append({
                        'warning_type': 'processing',
                        'warning_message': warning,
                        'timestamp': pd.Timestamp.now().isoformat()
                    })
                for rule in sorted(processor.skipped_rules):
                    warning_msg = (f"Rule {rule} skipped due to "
                                   f"missing fields")
                    warnings_data.append({
                        'warning_type': 'skipped_rule',
                        'warning_message': warning_msg,
                        'timestamp': pd.Timestamp.now().isoformat()
                    })
                
                if warnings_data:
                    self.warnings = pd.DataFrame(warnings_data)
                else:
                    self.warnings = pd.DataFrame()
                
                st.success("Data processed successfully!")
                return True
                
        except Exception as e:
            st.error(f"Data was not processed successfully: {e}")
            return False

    def run(self):
        """Run the Streamlit application."""
        # Setup page
        self.setup_page()
        
        # Load data
        self.load_data()
        
        # Render header
        self.render_header()
        
        # Data Source Info & Override in Sidebar
        with st.sidebar:
            st.subheader("üìÅ Data Source")
            
            # Get source description
            source_desc = "Unknown"
            if (self.config.app and self.config.app.input and
                    self.config.app.input.data):
                data_config = self.config.app.input.data
                if isinstance(data_config, DataInput):
                    if data_config.api_call:
                        source_desc = f"API: {data_config.api_call}"
                    elif data_config.file:
                        source_desc = f"File: {data_config.file}"
                elif isinstance(data_config, dict):
                    if data_config.get('api_call'):
                        source_desc = f"API: {data_config['api_call']}"
                    elif data_config.get('file'):
                        source_desc = f"File: {data_config['file']}"
                elif isinstance(data_config, str):
                    source_desc = f"File: {data_config}"

            if not self.data.empty:
                source_info_placeholder = st.empty()
                
                with st.expander("Upload New Raw Data"):
                    st.write("Upload a raw run data file to process and "
                             "replace the current data.")
                    uploaded_file = st.file_uploader(
                        "Upload Run Data (TSV)",
                        type=['tsv', 'txt', 'csv'],
                        key="sidebar_uploader"
                    )
                    
                    if uploaded_file:
                        if self._process_uploaded_file(uploaded_file):
                            source_desc = f"User Upload: {uploaded_file.name}"
                        else:
                            # If processing failed, clear the data so we don't
                            # show stale data with a confusing error state
                            self.data = pd.DataFrame()
                
                if not self.data.empty:
                    source_info_placeholder.info(
                        f"**Current Source:**\n{source_desc}"
                    )
            
            st.markdown("---")
        
        # Check if data is loaded (if empty and no upload in sidebar)
        if self.data.empty:
            st.info("No processed QC data found. Please upload a raw run "
                    "data file to process.")
            uploaded_file = st.file_uploader(
                "Upload Run Data (TSV)", type=['tsv', 'txt', 'csv'],
                key="main_uploader"
            )
            
            if uploaded_file:
                if not self._process_uploaded_file(uploaded_file):
                    st.stop()
            else:
                st.stop()

        # Get filtered data (this will also render sidebar metrics)
        filtered_data = self.render_sidebar_filters()
        
        # Main content tabs
        tabs = st.tabs([
            "üìä Data Preview",
            "üìà Overview",
            "üîç Quality Metrics",
            "üî¨ Sample Details",
            "‚öôÔ∏è QC Tests",
            "‚ö†Ô∏è Warnings"
        ])
        
        with tabs[0]:
            self.render_data_tab(filtered_data)
        
        with tabs[1]:
            self.render_overview_tab(filtered_data)
        
        with tabs[2]:
            self.render_quality_metrics_tab(filtered_data)
        
        with tabs[3]:
            self.render_sample_details_tab(filtered_data)
        
        with tabs[4]:
            self.render_qc_tests_tab()
        
        with tabs[5]:
            self.render_warnings_tab()


def _run_dashboard():
    """Internal function to run the dashboard logic."""
    import argparse
    
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='uQCme Dashboard')
    parser.add_argument(
        '--config',
        default=None,
        help='Path to configuration file'
    )
    
    try:
        args = parser.parse_args()
        config_path = args.config
    except SystemExit:
        # Streamlit might trigger this if --help is passed
        return

    # Logic to find config file
    if config_path:
        # User explicitly provided a path
        if not Path(config_path).exists():
            st.error(f"Configuration file '{config_path}' not found!")
            st.stop()
    else:
        # Try default locations
        possible_paths = ['config.yaml', 'config/config.yaml']
        found = False
        for path in possible_paths:
            if Path(path).exists():
                config_path = path
                found = True
                break
        
        if not found:
            # If no config found, we'll let QCDashboard load defaults
            # instead of stopping execution
            config_path = None
            st.info(
                "No configuration file found. Using default configuration."
            )
    
    # Initialize and run dashboard
    dashboard = QCDashboard(config_path)
    dashboard.run()


def main():
    """Entry point for the application script."""
    if st.runtime.exists():
        _run_dashboard()
    else:
        sys.argv = ["streamlit", "run", __file__] + sys.argv[1:]
        sys.exit(stcli.main())


if __name__ == "__main__":
    main()
